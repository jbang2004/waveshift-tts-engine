import re
import logging
import asyncio
from pathlib import Path
from typing import List, Dict, Tuple, Optional

try:
    from pydub import AudioSegment
except ImportError:
    AudioSegment = None
    print("âš ï¸ pydub æœªå®‰è£…ï¼ŒéŸ³é¢‘åˆ‡ç‰‡åŠŸèƒ½å°†ä¸å¯ç”¨")

from config import get_config
from core.sentence_tools import Sentence
from utils.path_manager import PathManager

logger = logging.getLogger(__name__)

class AudioSegmenter:
    """éŸ³é¢‘åˆ‡åˆ†æœåŠ¡ - åŸºäºè¯´è¯äººåˆ†ç»„çš„æ™ºèƒ½éŸ³é¢‘åˆ‡ç‰‡"""
    
    def __init__(self):
        self.config = get_config()
        self.logger = logging.getLogger(__name__)
        
        # éŸ³é¢‘åˆ‡ç‰‡é…ç½®
        self.goal_duration_ms = self.config.AUDIO_CLIP_GOAL_DURATION_MS
        self.min_duration_ms = self.config.AUDIO_CLIP_MIN_DURATION_MS
        self.padding_ms = self.config.AUDIO_CLIP_PADDING_MS
        self.allow_cross_non_speech = self.config.AUDIO_CLIP_ALLOW_CROSS_NON_SPEECH
        
        if AudioSegment is None:
            self.logger.error("pydub æœªå®‰è£…ï¼ŒéŸ³é¢‘åˆ‡ç‰‡åŠŸèƒ½å°†ä¸å¯ç”¨")
            raise ImportError("éœ€è¦å®‰è£… pydub åº“: pip install pydub")
            
        self.logger.info("éŸ³é¢‘åˆ‡åˆ†æœåŠ¡åˆå§‹åŒ–å®Œæˆ")
    
    def _time_str_to_ms(self, time_str: str) -> int:
        """æ—¶é—´å­—ç¬¦ä¸²è½¬æ¯«ç§’çš„è¾…åŠ©å‡½æ•°"""
        match = re.match(r'(\d+)m(\d+)s(\d+)ms', time_str)
        if not match: 
            return 0
        m, s, ms = map(int, match.groups())
        return m * 60 * 1000 + s * 1000 + ms
    
    def _ms_to_time_str(self, ms: float) -> str:
        """æ¯«ç§’è½¬æ—¶é—´å­—ç¬¦ä¸²çš„è¾…åŠ©å‡½æ•°"""
        ms_int = int(ms)  # è½¬æ¢ä¸ºæ•´æ•°
        minutes = ms_int // 60000
        seconds = (ms_int % 60000) // 1000
        milliseconds = ms_int % 1000
        return f"{minutes}m{seconds}s{milliseconds}ms"
    
    def _sentences_to_transcript_data(self, sentences: List[Sentence]) -> List[Dict]:
        """å°†Sentenceå¯¹è±¡è½¬æ¢ä¸ºå¤–éƒ¨audio.pyéœ€è¦çš„æ ¼å¼"""
        transcript_data = []
        for sentence in sentences:
            transcript_data.append({
                'sequence': sentence.sequence,
                'start': self._ms_to_time_str(sentence.start_ms),
                'end': self._ms_to_time_str(sentence.end_ms),
                'speaker': sentence.speaker,
                'original': sentence.original_text,
                'translation': sentence.translated_text,
                'content_type': 'speech'  # å‡è®¾æ‰€æœ‰å¥å­éƒ½æ˜¯speechç±»å‹
            })
        return transcript_data
    
    def _create_audio_clips(self, transcript_data: List[Dict]) -> Tuple[Dict, Dict]:
        """æ ¹æ®è½¬å½•æ•°æ®åˆ›å»ºéŸ³é¢‘åˆ‡ç‰‡è®¡åˆ’ï¼ˆæ”¯æŒpaddingè¿‡æ¸¡ï¼‰"""
        # é¢„å¤„ç†ï¼šåªå¤„ç†speechç±»å‹çš„å†…å®¹
        sentences = []
        for item in transcript_data:
            if item.get('content_type') != 'speech':
                continue
            
            start_ms = self._time_str_to_ms(item['start'])
            end_ms = self._time_str_to_ms(item['end'])
            
            # æ·»åŠ paddingï¼šå¼€å¤´å‡å»paddingï¼Œç»“å°¾åŠ ä¸Špadding
            padded_start = max(0, start_ms - self.padding_ms)
            padded_end = end_ms + self.padding_ms
            
            item['original_segment'] = [start_ms, end_ms]  # ä¿å­˜åŸå§‹æ—¶é—´æ®µ
            item['padded_segment'] = [padded_start, padded_end]  # å¸¦paddingçš„æ—¶é—´æ®µ
            item['segment_duration'] = padded_end - padded_start
            
            if item['segment_duration'] > 0:
                sentences.append(item)

        if not sentences:
            return {}, {}

        # è¯†åˆ«åŒè¯´è¯äººè¿ç»­å—
        large_blocks = []
        if sentences:
            current_block = [sentences[0]]
            for i in range(1, len(sentences)):
                current_sentence = sentences[i]
                last_sentence = current_block[-1]
                
                # æ£€æŸ¥è¯´è¯äººæ˜¯å¦ç›¸åŒ
                same_speaker = current_sentence['speaker'] == last_sentence['speaker']
                
                # æ£€æŸ¥åºåˆ—æ˜¯å¦è¿ç»­ï¼Œæˆ–è€…å¦‚æœå…è®¸è·¨è¶Šéspeechï¼Œåˆ™æ£€æŸ¥ä¸­é—´æ˜¯å¦æœ‰éspeechç‰‡æ®µ
                if same_speaker:
                    if self.allow_cross_non_speech:
                        # å…è®¸è·¨è¶Šéspeechç‰‡æ®µï¼Œç›´æ¥æ£€æŸ¥è¯´è¯äººç›¸åŒå³å¯
                        current_block.append(current_sentence)
                    else:
                        # ä¸å…è®¸è·¨è¶Šéspeechç‰‡æ®µï¼Œéœ€è¦æ£€æŸ¥åºåˆ—æ˜¯å¦ä¸¥æ ¼è¿ç»­
                        if current_sentence['sequence'] == last_sentence['sequence'] + 1:
                            current_block.append(current_sentence)
                        else:
                            # åºåˆ—ä¸è¿ç»­ï¼Œè¯´æ˜ä¸­é—´æœ‰éspeechç‰‡æ®µï¼Œå¼€å§‹æ–°å—
                            large_blocks.append(current_block)
                            current_block = [current_sentence]
                else:
                    large_blocks.append(current_block)
                    current_block = [current_sentence]
            large_blocks.append(current_block)

        # ç®€åŒ–çš„é€»è¾‘ï¼šæ¯ä¸ªlarge_blocksç”Ÿæˆä¸€ä¸ªclip
        clips_library = {}
        sentence_to_clip_id_map = {}
        clip_id_counter = 0

        for block in large_blocks:
            block_total_duration = sum(s['segment_duration'] for s in block)
            
            # åªå¤„ç†æ€»æ—¶é•¿å¤§äºç­‰äºmin_duration_msçš„å—
            if block_total_duration >= self.min_duration_ms:
                clip_id_counter += 1
                clip_id = f"Clip_{clip_id_counter}"
                
                # å¦‚æœæ€»æ—¶é•¿è¶…è¿‡goal_duration_msï¼Œéœ€è¦æˆªå–å‰goal_duration_msçš„éŸ³é¢‘
                if block_total_duration > self.goal_duration_ms:
                    # è®¡ç®—éœ€è¦æˆªå–çš„å¥å­
                    accumulated_duration = 0
                    sentences_to_include = []
                    
                    for sentence in block:
                        if accumulated_duration + sentence['segment_duration'] <= self.goal_duration_ms:
                            sentences_to_include.append(sentence)
                            accumulated_duration += sentence['segment_duration']
                        else:
                            # æ·»åŠ éƒ¨åˆ†å¥å­ä»¥è¾¾åˆ°goal_duration_ms
                            remaining_duration = self.goal_duration_ms - accumulated_duration
                            if remaining_duration > 0:
                                # åˆ›å»ºæˆªå–çš„å¥å­å‰¯æœ¬
                                truncated_sentence = sentence.copy()
                                # è°ƒæ•´æˆªå–å¥å­çš„segment_duration
                                truncated_sentence['segment_duration'] = remaining_duration
                                # è°ƒæ•´padded_segmentçš„ç»“æŸæ—¶é—´
                                start_time = truncated_sentence['padded_segment'][0]
                                truncated_sentence['padded_segment'] = [start_time, start_time + remaining_duration]
                                sentences_to_include.append(truncated_sentence)
                            break
                    
                    final_sentences = sentences_to_include
                else:
                    final_sentences = block
                
                # åˆå¹¶é‡å çš„segments
                merged_segments = self._merge_overlapping_segments(final_sentences)
                
                clips_library[clip_id] = {
                    "speaker": block[0]['speaker'],
                    "total_duration_ms": sum(end - start for start, end in merged_segments),
                    "segments_to_concatenate": merged_segments,
                    "padding_ms": self.padding_ms,
                    "sentences": [{
                        "sequence": s['sequence'], 
                        "original": s['original'], 
                        "translation": s['translation'],
                        "original_time": s['original_segment']
                    } for s in final_sentences]
                }
                
                # ä¸ºæ‰€æœ‰åŸå§‹å¥å­æ˜ å°„clip_idï¼ˆåŒ…æ‹¬æœªæˆªå–çš„éƒ¨åˆ†ï¼‰
                for sentence in block:
                    sentence_to_clip_id_map[sentence['sequence']] = clip_id

        return clips_library, sentence_to_clip_id_map
    
    def _merge_overlapping_segments(self, block: List[Dict]) -> List[List[int]]:
        """åˆå¹¶é‡å çš„segmentsï¼Œç¡®ä¿å¹³æ»‘è¿‡æ¸¡"""
        if not block:
            return []
        
        segments = []
        for sentence in block:
            segments.append(sentence['padded_segment'])
        
        # æŒ‰å¼€å§‹æ—¶é—´æ’åº
        segments.sort(key=lambda x: x[0])
        
        merged = [segments[0]]
        for current in segments[1:]:
            last = merged[-1]
            
            # å¦‚æœå½“å‰segmentä¸ä¸Šä¸€ä¸ªé‡å ï¼Œåˆå¹¶å®ƒä»¬
            if current[0] <= last[1]:
                merged[-1] = [last[0], max(last[1], current[1])]
            else:
                merged.append(current)
        
        return merged
    
    async def _extract_and_save_audio_clips(self, audio_path: str, clips_library: Dict, 
                                          output_dir: str) -> Dict[str, str]:
        """å¹¶è¡Œç‰ˆæœ¬çš„éŸ³é¢‘åˆ‡ç‰‡æå–"""
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        audio_file = Path(audio_path)
        if not audio_file.exists():
            error_msg = f"éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}"
            self.logger.error(f"âŒ {error_msg}")
            raise FileNotFoundError(error_msg)
        
        if not audio_file.is_file():
            error_msg = f"è·¯å¾„ä¸æ˜¯æ–‡ä»¶: {audio_path}"
            self.logger.error(f"âŒ {error_msg}")
            raise ValueError(error_msg)
        
        # å¼‚æ­¥åŠ è½½éŸ³é¢‘æ–‡ä»¶
        file_size = audio_file.stat().st_size
        self.logger.info(f"ğŸµ åŠ è½½éŸ³é¢‘æ–‡ä»¶: {audio_path} (å¤§å°: {file_size/1024/1024:.1f}MB)")
        
        try:
            audio = await asyncio.to_thread(AudioSegment.from_file, audio_path)
            self.logger.info(f"âœ… éŸ³é¢‘æ–‡ä»¶åŠ è½½æˆåŠŸï¼Œæ—¶é•¿: {len(audio)/1000:.1f}ç§’")
        except Exception as e:
            error_msg = f"åŠ è½½éŸ³é¢‘æ–‡ä»¶å¤±è´¥: {e}"
            self.logger.error(f"âŒ {error_msg}")
            raise RuntimeError(error_msg) from e
        
        # å¹¶è¡Œå¤„ç†æ‰€æœ‰åˆ‡ç‰‡
        async def process_single_clip(clip_id: str, clip_info: Dict) -> Tuple[str, Optional[str]]:
            """å¤„ç†å•ä¸ªéŸ³é¢‘åˆ‡ç‰‡"""
            try:
                padding_ms = clip_info['padding_ms']
                self.logger.info(f"ğŸ¬ å¤„ç† {clip_id}: {clip_info['speaker']} ({clip_info['total_duration_ms']/1000:.1f}ç§’) [padding: {padding_ms}ms]")
                
                # åˆå¹¶æ‰€æœ‰ç‰‡æ®µï¼Œä½¿ç”¨paddingè¿›è¡Œå¹³æ»‘è¿‡æ¸¡
                combined_audio = AudioSegment.empty()
                segments_to_process = clip_info['segments_to_concatenate']
                
                for i, (start_ms, end_ms) in enumerate(segments_to_process):
                    # è¾¹ç•Œæ£€æŸ¥
                    if start_ms < 0:
                        start_ms = 0
                    if end_ms > len(audio):
                        end_ms = len(audio)
                    if start_ms >= end_ms:
                        continue
                        
                    segment = audio[start_ms:end_ms]
                    
                    # ä½¿ç”¨paddingå®ç°å¹³æ»‘è¿‡æ¸¡
                    if len(segment) > padding_ms * 2:
                        # ä¸ºäº†é¿å…çªç„¶çš„å¼€å§‹å’Œç»“æŸï¼Œåœ¨paddingåŒºåŸŸåº”ç”¨æ·¡å…¥æ·¡å‡º
                        fade_duration = min(padding_ms // 2, 100)  # æ·¡å…¥æ·¡å‡ºæ—¶é•¿
                        
                        if i == 0:
                            # ç¬¬ä¸€ä¸ªsegmentï¼šåœ¨å¼€å¤´åº”ç”¨æ·¡å…¥
                            segment = segment.fade_in(fade_duration)
                        
                        if i == len(segments_to_process) - 1:
                            # æœ€åä¸€ä¸ªsegmentï¼šåœ¨ç»“å°¾åº”ç”¨æ·¡å‡º
                            segment = segment.fade_out(fade_duration)
                        else:
                            # ä¸­é—´çš„segmentsï¼šä¸¤ç«¯éƒ½è¿›è¡Œè½»å¾®çš„æ·¡å…¥æ·¡å‡ºä»¥ç¡®ä¿å¹³æ»‘
                            segment = segment.fade_in(fade_duration // 2).fade_out(fade_duration // 2)
                    
                    combined_audio += segment
                
                if len(combined_audio) == 0:
                    self.logger.warning(f"   âš ï¸ {clip_id} ç‰‡æ®µä¸ºç©ºï¼Œè·³è¿‡")
                    return clip_id, None
                
                # ä¿å­˜éŸ³é¢‘ç‰‡æ®µ
                speaker_name = clip_info['speaker'].replace(' ', '_').replace('/', '_')
                clip_filename = f"{clip_id}_{speaker_name}.wav"
                clip_filepath = output_path / clip_filename
                
                # æ·»åŠ æœ€ç»ˆçš„éŸ³é¢‘æ ‡å‡†åŒ–å¹¶å¼‚æ­¥ä¿å­˜
                combined_audio = combined_audio.normalize()
                await asyncio.to_thread(combined_audio.export, str(clip_filepath), format="wav")
                
                self.logger.info(f"   âœ… å·²ä¿å­˜: {clip_filepath}")
                return clip_id, str(clip_filepath)
                
            except Exception as e:
                self.logger.error(f"å¤„ç†åˆ‡ç‰‡ {clip_id} å¤±è´¥: {e}")
                return clip_id, None
        
        # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰åˆ‡ç‰‡å¤„ç†
        self.logger.info(f"ğŸš€ å¼€å§‹å¹¶è¡Œå¤„ç† {len(clips_library)} ä¸ªéŸ³é¢‘åˆ‡ç‰‡")
        tasks = [process_single_clip(clip_id, clip_info) for clip_id, clip_info in clips_library.items()]
        results = await asyncio.gather(*tasks)
        
        # æ”¶é›†æˆåŠŸçš„ç»“æœ
        clip_files = {clip_id: filepath for clip_id, filepath in results if filepath}
        
        self.logger.info(f"âœ… å¹¶è¡Œå¤„ç†å®Œæˆï¼ŒæˆåŠŸç”Ÿæˆ {len(clip_files)} ä¸ªéŸ³é¢‘åˆ‡ç‰‡")
        return clip_files
    
    def _map_clips_to_sentences(self, sentences: List[Sentence], clips_library: Dict, 
                              clip_files: Dict, sentence_to_clip_id_map: Dict) -> List[Sentence]:
        """å°†åˆ‡ç‰‡æ˜ å°„å›å¥å­å¯¹è±¡"""
        updated_sentences = []
        
        for sentence in sentences:
            # è·å–å¥å­å¯¹åº”çš„clip_id
            clip_id = sentence_to_clip_id_map.get(sentence.sequence)
            
            if clip_id and clip_id in clip_files:
                # æ›´æ–°å¥å­çš„éŸ³é¢‘è·¯å¾„
                sentence.audio = clip_files[clip_id]
                
                # è·å–clipä¿¡æ¯
                clip_info = clips_library[clip_id]
                
                # è®¡ç®—å®é™…çš„éŸ³é¢‘æ—¶é•¿ï¼ˆç”¨äºè¯­éŸ³å…‹éš†å‚è€ƒï¼‰
                sentence.speech_duration = clip_info['total_duration_ms'] / 1000.0
                
                self.logger.debug(f"å¥å­ {sentence.sequence} æ˜ å°„åˆ°åˆ‡ç‰‡ {clip_id}")
            else:
                self.logger.warning(f"å¥å­ {sentence.sequence} æœªæ‰¾åˆ°å¯¹åº”çš„éŸ³é¢‘åˆ‡ç‰‡")
            
            updated_sentences.append(sentence)
        
        return updated_sentences
    
    async def segment_audio_for_sentences(self, task_id: str, audio_file_path: str, 
                                        sentences: List[Sentence], path_manager=None) -> List[Sentence]:
        """
        ä¸ºå¥å­åˆ—è¡¨åˆ‡åˆ†éŸ³é¢‘ï¼ŒåŸºäºè¯´è¯äººåˆ†ç»„çš„æ™ºèƒ½åˆ‡ç‰‡
        
        Args:
            task_id: ä»»åŠ¡ID
            audio_file_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            sentences: å¥å­åˆ—è¡¨
            path_manager: å…±äº«çš„è·¯å¾„ç®¡ç†å™¨ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            List[Sentence]: æ›´æ–°äº†éŸ³é¢‘è·¯å¾„çš„å¥å­åˆ—è¡¨
        """
        try:
            self.logger.info(f"[{task_id}] å¼€å§‹ä¸º {len(sentences)} ä¸ªå¥å­è¿›è¡Œæ™ºèƒ½éŸ³é¢‘åˆ‡ç‰‡")
            
            # è½¬æ¢æ•°æ®æ ¼å¼
            transcript_data = self._sentences_to_transcript_data(sentences)
            self.logger.info(f"[{task_id}] è½¬æ¢äº† {len(transcript_data)} ä¸ªè½¬å½•ç‰‡æ®µ")
            
            # ç”Ÿæˆåˆ‡ç‰‡è®¡åˆ’
            clips_library, sentence_to_clip_id_map = self._create_audio_clips(transcript_data)
            
            if not clips_library:
                self.logger.warning(f"[{task_id}] æœªèƒ½ç”Ÿæˆæœ‰æ•ˆçš„éŸ³é¢‘åˆ‡ç‰‡")
                return sentences
            
            self.logger.info(f"[{task_id}] ç”Ÿæˆäº† {len(clips_library)} ä¸ªéŸ³é¢‘åˆ‡ç‰‡")
            
            # ä½¿ç”¨ä¼ å…¥çš„path_managerï¼Œå¦‚æœæ²¡æœ‰åˆ™åˆ›å»ºæ–°çš„ï¼ˆå‘åå…¼å®¹ï¼‰
            if path_manager is None:
                path_manager = PathManager(task_id)
                self.logger.warning(f"[{task_id}] AudioSegmenter: æœªä¼ å…¥path_managerï¼Œåˆ›å»ºæ–°çš„ä¸´æ—¶ç›®å½•")
            
            audio_clips_dir = path_manager.temp.audio_prompts_dir
            
            # æå–å¹¶ä¿å­˜éŸ³é¢‘åˆ‡ç‰‡
            try:
                clip_files = await self._extract_and_save_audio_clips(
                    audio_file_path, clips_library, str(audio_clips_dir)
                )
                
                if not clip_files:
                    error_msg = f"éŸ³é¢‘åˆ‡ç‰‡æå–å¤±è´¥ï¼šæœªç”Ÿæˆä»»ä½•åˆ‡ç‰‡æ–‡ä»¶"
                    self.logger.error(f"[{task_id}] {error_msg}")
                    raise ValueError(error_msg)
                    
            except Exception as e:
                error_msg = f"éŸ³é¢‘åˆ‡ç‰‡æå–å¼‚å¸¸: {e}"
                self.logger.error(f"[{task_id}] {error_msg}")
                raise RuntimeError(error_msg) from e
            
            # æ˜ å°„åˆ‡ç‰‡åˆ°å¥å­
            updated_sentences = self._map_clips_to_sentences(
                sentences, clips_library, clip_files, sentence_to_clip_id_map
            )
        
            successful_clips = len([s for s in updated_sentences if s.audio])
            self.logger.info(f"[{task_id}] æ™ºèƒ½éŸ³é¢‘åˆ‡ç‰‡å®Œæˆï¼ŒæˆåŠŸå¤„ç† {successful_clips} ä¸ªå¥å­")
            
            return updated_sentences
        except Exception as e:
            self.logger.error(f"[{task_id}] éŸ³é¢‘åˆ‡ç‰‡å¤„ç†å¤±è´¥: {e}")
            raise